
\documentclass[letter, 9pt]{article}

\title{Lecture One}
\begin{document}
\maketitle
\section{Machine Learning Intro}
The first section of the class went over the description of machine learning and how it is decomposed in mathematics. As well different areas of uses were described.

\subsection{KNN}
At first Nearest-Neighbors search was discussed as a method for deciphering the difference between classes. It is a method for classification.

The use of KNN is a way of speeding up the classification step. With the use of data-structures one can avoid the comptational intensity of NNS.

\subsubsection{KD-Trees}
With the use of a tree structure you can speed of KNNS significantly. It has a few disadvantages that make queries not completely accurate.

The limitations of the KD-tree for the nearest point is discussed. The method of checking candidate selection by traversing back to the the root of the tree and finding new candidates by traversing down to the leaves of the tree.
\being{itemize}
\item best-case: $log(N)$
\item worst-case: $N$

\subsection{Bayes vs NN Classifier}
NN classifier is at most twice as bad as the optimal classifier.
\end{document}